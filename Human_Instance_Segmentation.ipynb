{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human Instance Segmentation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "t_iHs_wm2Mhh"
      },
      "cell_type": "markdown",
      "source": [
        "# Human Instance Segmentation Demo\n",
        "\n",
        "This Colab enables you to use a Mask R-CNN model that was trained on Cloud TPU to perform instance segmentation on a sample input image. The resulting predictions are overlayed on the sample image as boxes, instance masks, and labels. You can also experiment with your own images by editing the input image URL.\n",
        "\n",
        "### About Mask R-CNN\n",
        "The Mask R-CNN model addresses one of the most difficult computer vision challenges: image segmentation. Image segmentation is the task of detecting and distinguishing multiple objects within a single image. In particular, Mask R-CNN performs \"instance segmentation,\" which means that different instances of the same type of object in the input image, for example, car, should be assigned distinct labels."
      ]
    },
    {
      "metadata": {
        "id": "DL_0tSC67biu"
      },
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "<h3><a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>  &nbsp;&nbsp;Use a free Cloud TPU</h3>\n",
        " \n",
        "   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "   2. Click Runtime again and select **Runtime > Run All**. You can also run the cells manually with Shift-ENTER."
      ]
    },
    {
      "metadata": {
        "id": "ODxpKwkNFrBk"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the source code\n",
        "Download the source code of the Mask R-CNN model."
      ]
    },
    {
      "metadata": {
        "id": "rUaCGBUQFgiq",
        "outputId": "b85a3e3a-dc46-4512-96b4-5a9cbc21d210",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/tpu/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tpu'...\n",
            "remote: Enumerating objects: 10497, done.\u001b[K\n",
            "remote: Counting objects: 100% (699/699), done.\u001b[K\n",
            "remote: Compressing objects: 100% (442/442), done.\u001b[K\n",
            "remote: Total 10497 (delta 339), reused 486 (delta 243), pack-reused 9798\u001b[K\n",
            "Receiving objects: 100% (10497/10497), 45.52 MiB | 28.49 MiB/s, done.\n",
            "Resolving deltas: 100% (7377/7377), done.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "RPbB7s8hPHfO"
      },
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ]
    },
    {
      "metadata": {
        "id": "6EtGbyNc8VgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd21cd0f-37a1-45af-ceee-f92036b2ddd7"
      },
      "cell_type": "code",
      "source": [
        "from IPython import display\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "sys.path.insert(0, 'tpu/models/official')\n",
        "sys.path.insert(0, 'tpu/models/official/mask_rcnn')\n",
        "import coco_metric\n",
        "from mask_rcnn.object_detection import visualization_utils"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "vkTSG22ePKkz"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the COCO index mapping\n",
        "This Colab uses a pretrained checkpoint of the Mask R-CNN model that is trained using the COCO dataset. Here is the mapping between the indices that the model predicts and the categories in text."
      ]
    },
    {
      "metadata": {
        "id": "_Q5r1zob93OF"
      },
      "cell_type": "code",
      "source": [
        "ID_MAPPING = {\n",
        "    1: 'person',\n",
        "    2: 'bicycle',\n",
        "    3: 'car',\n",
        "    4: 'motorcycle',\n",
        "    5: 'airplane',\n",
        "    6: 'bus',\n",
        "    7: 'train',\n",
        "    8: 'truck',\n",
        "    9: 'boat',\n",
        "    10: 'traffic light',\n",
        "    11: 'fire hydrant',\n",
        "    13: 'stop sign',\n",
        "    14: 'parking meter',\n",
        "    15: 'bench',\n",
        "    16: 'bird',\n",
        "    17: 'cat',\n",
        "    18: 'dog',\n",
        "    19: 'horse',\n",
        "    20: 'sheep',\n",
        "    21: 'cow',\n",
        "    22: 'elephant',\n",
        "    23: 'bear',\n",
        "    24: 'zebra',\n",
        "    25: 'giraffe',\n",
        "    27: 'backpack',\n",
        "    28: 'umbrella',\n",
        "    31: 'handbag',\n",
        "    32: 'tie',\n",
        "    33: 'suitcase',\n",
        "    34: 'frisbee',\n",
        "    35: 'skis',\n",
        "    36: 'snowboard',\n",
        "    37: 'sports ball',\n",
        "    38: 'kite',\n",
        "    39: 'baseball bat',\n",
        "    40: 'baseball glove',\n",
        "    41: 'skateboard',\n",
        "    42: 'surfboard',\n",
        "    43: 'tennis racket',\n",
        "    44: 'bottle',\n",
        "    46: 'wine glass',\n",
        "    47: 'cup',\n",
        "    48: 'fork',\n",
        "    49: 'knife',\n",
        "    50: 'spoon',\n",
        "    51: 'bowl',\n",
        "    52: 'banana',\n",
        "    53: 'apple',\n",
        "    54: 'sandwich',\n",
        "    55: 'orange',\n",
        "    56: 'broccoli',\n",
        "    57: 'carrot',\n",
        "    58: 'hot dog',\n",
        "    59: 'pizza',\n",
        "    60: 'donut',\n",
        "    61: 'cake',\n",
        "    62: 'chair',\n",
        "    63: 'couch',\n",
        "    64: 'potted plant',\n",
        "    65: 'bed',\n",
        "    67: 'dining table',\n",
        "    70: 'toilet',\n",
        "    72: 'tv',\n",
        "    73: 'laptop',\n",
        "    74: 'mouse',\n",
        "    75: 'remote',\n",
        "    76: 'keyboard',\n",
        "    77: 'cell phone',\n",
        "    78: 'microwave',\n",
        "    79: 'oven',\n",
        "    80: 'toaster',\n",
        "    81: 'sink',\n",
        "    82: 'refrigerator',\n",
        "    84: 'book',\n",
        "    85: 'clock',\n",
        "    86: 'vase',\n",
        "    87: 'scissors',\n",
        "    88: 'teddy bear',\n",
        "    89: 'hair drier',\n",
        "    90: 'toothbrush',\n",
        "}\n",
        "category_index = {k: {'id': k, 'name': ID_MAPPING[k]} for k in ID_MAPPING}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HngQNsyjXmvF"
      },
      "cell_type": "markdown",
      "source": [
        "## Load an image\n",
        "Now, you can load an image. Use either the sample image included here, or update the field with an image of your choice."
      ]
    },
    {
      "metadata": {
        "id": "uwrFI4C9Vvam"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a Tensorflow session\n",
        "Now let us create a Tensorflow session to run the inference. You can either connect to a TPU or a normal CPU backend."
      ]
    },
    {
      "metadata": {
        "id": "X0G-tk6wakcr",
        "outputId": "6f428015-dd8b-4bc6-beb6-05393b63647a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "use_tpu = True #@param {type:\"boolean\"}\n",
        "if use_tpu:\n",
        "  import os\n",
        "  import pprint\n",
        "\n",
        "  assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "  session = tf.Session(TPU_ADDRESS, graph=tf.Graph())\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "else:\n",
        "  session = tf.Session(graph=tf.Graph())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU address is grpc://10.18.70.82:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 14515720121401774327),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 1259318930450700018),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1876357766553890743),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9659897369616980407),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5053114798325246217),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 13713953648169066472),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5999331226335247101),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 12110879340991507052),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6931035752283100990),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 9000105191580761380),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 14088857127584215528)]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "YtXyXw6EaKRj"
      },
      "cell_type": "markdown",
      "source": [
        "# Load the pretrained model\n",
        "Loading the COCO pretrained saved model from the public GCS bucket. "
      ]
    },
    {
      "metadata": {
        "id": "6lCL-ZcwaJbA",
        "outputId": "234e7c68-8c98-449d-9e65-170e793ef714",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "saved_model_dir = 'gs://cloud-tpu-checkpoints/mask-rcnn/1555659850' #@param {type:\"string\"}\n",
        "_ = tf.saved_model.loader.load(session, ['serve'], saved_model_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-9cb25c67e8d4>:2: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
            "INFO:tensorflow:Restoring parameters from gs://cloud-tpu-checkpoints/mask-rcnn/1555659850/variables/variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from PIL import Image, ImageColor\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "bsvs5iEbBIcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_mask_on_image_array(image, mask, color='red', alpha=0.4):\n",
        "  \"\"\"Draws mask on an image.\n",
        "  Args:\n",
        "    image: uint8 numpy array with shape (img_height, img_height, 3)\n",
        "    mask: a uint8 numpy array of shape (img_height, img_height) with\n",
        "      values between either 0 or 1.\n",
        "    color: color to draw the keypoints with. Default is red.\n",
        "    alpha: transparency value between 0 and 1. (default: 0.4)\n",
        "  Raises:\n",
        "    ValueError: On incorrect data type for image or masks.\n",
        "  \"\"\"\n",
        "  if image.dtype != np.uint8:\n",
        "    raise ValueError('`image` not of type np.uint8')\n",
        "  if mask.dtype != np.uint8:\n",
        "    raise ValueError('`mask` not of type np.uint8')\n",
        "  if np.any(np.logical_and(mask != 1, mask != 0)):\n",
        "    raise ValueError('`mask` elements should be in [0, 1]')\n",
        "  if image.shape[:2] != mask.shape:\n",
        "    raise ValueError('The image has spatial dimensions %s but the mask has '\n",
        "                     'dimensions %s' % (image.shape[:2], mask.shape))\n",
        "  if type(color) == str:\n",
        "    rgb = ImageColor.getrgb(color)\n",
        "  else:\n",
        "    rgb = color\n",
        "\n",
        "  pil_image = Image.fromarray(image)\n",
        "\n",
        "  solid_color = np.expand_dims(\n",
        "      np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n",
        "  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert('RGBA')\n",
        "  pil_mask = Image.fromarray(np.uint8(255.0*alpha*mask)).convert('L')\n",
        "  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
        "  np.copyto(image, np.array(pil_image.convert('RGB')))"
      ],
      "metadata": {
        "id": "NdTdtSzUTptg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2oZWLz4xXsyQ"
      },
      "cell_type": "code",
      "source": [
        "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Kitano_Street_Kobe01s5s4110.jpg/2560px-Kitano_Street_Kobe01s5s4110.jpg -O test.jpg\n",
        "image_path = 'test.jpg'\n",
        "\n",
        "with open(image_path, 'rb') as f:\n",
        "  np_image_string = np.array([f.read()])\n",
        "  \n",
        "image = Image.open(image_path)\n",
        "width, height = image.size\n",
        "np_image = np.array(image.getdata()).reshape(height, width, 3).astype(np.uint8)\n",
        "\n",
        "display.display(display.Image(image_path, width=1024))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cjHuO49maf6R"
      },
      "cell_type": "markdown",
      "source": [
        "## Perform instance segmentation and retrieve the predictions\n",
        "Now let's run the inference and process the predictions from the model.\n"
      ]
    },
    {
      "metadata": {
        "id": "qENTOHzZcaWC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "296b7650-28ed-4096-df69-8f00ad328f20"
      },
      "cell_type": "code",
      "source": [
        "num_detections, detection_boxes, detection_classes, detection_scores, detection_masks, image_info = session.run(\n",
        "    ['NumDetections:0', 'DetectionBoxes:0', 'DetectionClasses:0', 'DetectionScores:0', 'DetectionMasks:0', 'ImageInfo:0'],\n",
        "    feed_dict={'Placeholder:0': np_image_string})\n",
        "\n",
        "num_detections = np.squeeze(num_detections.astype(np.int32), axis=(0,))\n",
        "detection_boxes = np.squeeze(detection_boxes * image_info[0, 2], axis=(0,))[0:num_detections]\n",
        "detection_scores = np.squeeze(detection_scores, axis=(0,))[0:num_detections]\n",
        "detection_classes = np.squeeze(detection_classes.astype(np.int32), axis=(0,))[0:num_detections]\n",
        "instance_masks = np.squeeze(detection_masks, axis=(0,))[0:num_detections]\n",
        "ymin, xmin, ymax, xmax = np.split(detection_boxes, 4, axis=-1)\n",
        "processed_boxes = np.concatenate([xmin, ymin, xmax - xmin, ymax - ymin], axis=-1)\n",
        "segmentations = coco_metric.generate_segmentation_from_masks(instance_masks, processed_boxes, height, width)\n",
        "\n",
        "# visualize\n",
        "max_boxes_to_draw = 50   #@param {type:\"integer\"}\n",
        "min_score_thresh = 0.2    #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "empty_img = np.zeros(np_image.shape, dtype=np.uint8)\n",
        "\n",
        "box_to_instance_masks_map = {}\n",
        "box_to_display_str_map = collections.defaultdict(list)\n",
        "boxes = detection_boxes\n",
        "scores = detection_scores\n",
        "box_to_color_map = collections.defaultdict(str)\n",
        "groundtruth_box_visualization_color = 'black'\n",
        "skip_labels = True\n",
        "instance_masks = segmentations\n",
        "\n",
        "for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
        "    if scores is None or scores[i] > min_score_thresh and detection_classes[i] == 1: # person only\n",
        "      box = tuple(boxes[i].tolist())\n",
        "      if instance_masks is not None:\n",
        "        box_to_instance_masks_map[box] = instance_masks[i]\n",
        "      if scores is None:\n",
        "        box_to_color_map[box] = groundtruth_box_visualization_color\n",
        "      else:\n",
        "        display_str = ''\n",
        "        box_to_display_str_map[box].append(display_str)\n",
        "        box_to_color_map[box] = (2, 2, 2) #'DarkOrange'\n",
        "\n",
        "for box, color in box_to_color_map.items():\n",
        "    ymin, xmin, ymax, xmax = box\n",
        "    if instance_masks is not None:\n",
        "      draw_mask_on_image_array(\n",
        "          empty_img,\n",
        "          box_to_instance_masks_map[box],\n",
        "          color=color\n",
        "      )\n",
        "\n",
        "image_with_detections = empty_img\n",
        "output_image_path = osp.splitext(image_path)[0]+'.png'\n",
        "Image.fromarray(image_with_detections.astype(np.uint8)).save(output_image_path)\n",
        "files.download(output_image_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d89015e4-1a89-441a-b193-f0cc68da15c7\", \"100006.png\", 10729)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "hh-aS5Iz-SiQ"
      },
      "cell_type": "markdown",
      "source": [
        "# What's next\n",
        "\n",
        "* Learn about [Cloud TPUs](https://cloud.google.com/tpu/docs) that Google designed and optimized specifically to speed up and scale up ML workloads for training and inference and to enable ML engineers and researchers to iterate more quickly.\n",
        "* Explore the range of [Cloud TPU tutorials and Colabs](https://cloud.google.com/tpu/docs/tutorials) to find other examples that can be used when implementing your ML project.\n",
        "* [Here's](https://cloud.google.com/tpu/docs/tutorials/mask-rcnn) a direct link to the Mask R-CNN tutorial.\n"
      ]
    }
  ]
}